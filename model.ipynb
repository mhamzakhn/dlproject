{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11424327,"sourceType":"datasetVersion","datasetId":7154849},{"sourceId":11437567,"sourceType":"datasetVersion","datasetId":7164361},{"sourceId":11509987,"sourceType":"datasetVersion","datasetId":7217211},{"sourceId":11603654,"sourceType":"datasetVersion","datasetId":7277748},{"sourceId":11603667,"sourceType":"datasetVersion","datasetId":7277760}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport os\nfrom PIL import Image\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom torch.cuda.amp import GradScaler, autocast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:15.840034Z","iopub.execute_input":"2025-04-26T23:09:15.840327Z","iopub.status.idle":"2025-04-26T23:09:23.376364Z","shell.execute_reply.started":"2025-04-26T23:09:15.840303Z","shell.execute_reply":"2025-04-26T23:09:23.375813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the device (GPU or CPU)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n################################# Input here ###################################\nroot_path='/kaggle/input/gromo-mustard-dataset/dataset/content/drive/MyDrive/ACM grand challenge/Crops data/For_age_prediction/'  # change to root dir of plant\ncrop='mustard'  # change to plant type\ncsv_file='/kaggle/input/gromo-ground-truths/Ground Truth/mustard_train.csv'\nn_images=4\nepochs=10\nplant_input=3\ndays_input=47\nbatch_size = 8\nseed=42\nheight, width = 224, 224\n# Transformations for resizing and converting to tensor\ntransform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n##############################################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.377447Z","iopub.execute_input":"2025-04-26T23:09:23.377859Z","iopub.status.idle":"2025-04-26T23:09:23.463054Z","shell.execute_reply.started":"2025-04-26T23:09:23.377836Z","shell.execute_reply":"2025-04-26T23:09:23.462322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    \"\"\"\n    Set random seeds for Python, NumPy, and PyTorch to ensure reproducibility.\n    \"\"\"\n    random.seed(seed)  # Python random seed\n    np.random.seed(seed)  # Numpy random seed\n    torch.manual_seed(seed)  # PyTorch CPU seed\n    torch.cuda.manual_seed(seed)  # PyTorch GPU seed (for CUDA)\n    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n\n    # Ensure deterministic behavior in CUDA (if using GPU)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# Initialize seed for reproducibility\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.463853Z","iopub.execute_input":"2025-04-26T23:09:23.464139Z","iopub.status.idle":"2025-04-26T23:09:23.622291Z","shell.execute_reply.started":"2025-04-26T23:09:23.464113Z","shell.execute_reply":"2025-04-26T23:09:23.621679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_channels_per_view = 6  # 3 RGB + 3 mask/leafcount/avg_area = 6 channels per view\ninput_channels_total = n_images * input_channels_per_view\npatch_size = 16\nprojection_dim = 256\nnum_heads = 8\nnum_layers = 6\nmlp_dim = 512\ndropout_rate = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.623969Z","iopub.execute_input":"2025-04-26T23:09:23.624177Z","iopub.status.idle":"2025-04-26T23:09:23.628229Z","shell.execute_reply.started":"2025-04-26T23:09:23.624161Z","shell.execute_reply":"2025-04-26T23:09:23.627674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CropDataset(Dataset):\n    def __init__(self, root_dir, csv_file, images_per_level, crop, plants, days,\n                 levels=['L1','L2','L3','L4','L5'], transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Directory with all the images.\n            csv_file (str): Path to the CSV file containing ground truth (filename, leaf_count, age).\n            images_per_level (int): Number of images to select per level (should be factors of 24).\n            crop (str): Crop type (e.g., \"radish\").\n            plants (int): Number of plants (e.g., 4).\n            days (int): Number of days (e.g., 59).\n            levels (list): List of levels (e.g., ['L1', 'L2', 'L3', 'L4', 'L5']).\n            transform (callable, optional): Transform to be applied on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        self.csv_file = csv_file\n        self.images_per_level = images_per_level\n        self.crop = crop\n        self.plants_num = plants\n        self.max_days = days\n        self.levels = levels\n        self.transform = transform\n        self.image_data = self._load_metadata()\n        self.image_paths = self._load_image_paths()\n\n    def _load_metadata(self):\n        \"\"\"Load CSV file into a pandas DataFrame and map filenames to leaf counts and ages.\"\"\"\n        df = pd.read_csv(self.csv_file)\n        df[\"filename\"] = df[\"filename\"].astype(str)  # Ensure filenames are strings\n        return df.set_index(\"filename\")  # Use filename as the index for quick lookup\n\n    def _select_angles(self):\n        \"\"\"\n        Select angles dynamically for a given level.\n        \"\"\"\n        images_needed = self.images_per_level\n        selected_angles = [i for i in range(0, 360, int(360 / images_needed))]\n\n        initial_angles = [i for i in range(15, selected_angles[1], 15)]\n        multiple_selections = [selected_angles]\n\n        for initial_angle in initial_angles:\n            selection = [initial_angle]\n            while len(selection) < images_needed:\n                next_angle = (selection[-1] + int(360 / images_needed)) % 360\n                if next_angle not in selection:\n                    selection.append(next_angle)\n            multiple_selections.append(selection)\n        print(multiple_selections)\n        return multiple_selections\n\n    def _load_image_paths(self):\n        \"\"\"\n        Load image paths for all levels and plants based on the selection of angles.\n        \"\"\"\n        image_paths = []\n        multiple_selections = self._select_angles()\n\n        for plant in range(1, self.plants_num + 1):\n            plant_path = os.path.join(self.root_dir, crop, f\"p{plant}\")\n            if not os.path.isdir(plant_path):\n                print(f\"Plant directory not found: {plant_path}\")\n                continue\n            for day in range(1, self.max_days + 1):\n                day_path = os.path.join(self.root_dir, crop, f\"p{plant}\", f\"d{day}\")\n                if not os.path.isdir(day_path):\n                    continue\n                for selected_angles in multiple_selections:\n                    for level in self.levels:\n                        level_path = os.path.join(self.root_dir,self.crop, f\"p{plant}\", f\"d{day}\", level)\n                        level_image_paths = [\n                            os.path.join(level_path, f\"{self.crop}_p{plant}_d{day}_{level}_{angle}.png\")\n                            for angle in selected_angles\n                        ]\n                        filename = os.path.join(self.crop,f\"p{plant}\", f\"d{day}\", level,f\"{self.crop}_p{plant}_d{day}_{level}_{selected_angles[0]}.png\")\n                        if filename not in self.image_data.index:\n                            continue\n                        leaf_count = self.image_data.loc[filename, \"leaf_count\"]\n                        # print(level_image_paths)\n                        image_paths.append((level_image_paths, leaf_count,day))  # Append day number along with image paths\n\n        print(f\"Total samples loaded: {len(image_paths)}\")\n        return image_paths\n\n\n    def __len__(self):\n        return len(self.image_paths)\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get a batch of images from the dataset corresponding to the angles selected.\n        \"\"\"\n        images = []\n        leaf_count = self.image_paths[idx][1]\n        age = self.image_paths[idx][2]\n        all_images= self.image_paths[idx][0]\n        \n        for img_path in all_images:  # Get the image paths for this sample\n            img = Image.open(img_path)\n            if self.transform:\n              img = self.transform(img)\n            \n            mask_tensor = torch.zeros((1, 224, 224), dtype=torch.float32)\n            num_leaves = 0\n            avg_area = 0.0\n\n            if \"/p1/\" in img_path or \"\\\\p1\\\\\" in img_path:\n                mask_prefix = \"p1-masks/p1\"\n            elif \"/p2/\" in img_path or \"\\\\p2\\\\\" in img_path:\n                mask_prefix = \"p2-masks/p2\"\n            elif \"/p3/\" in img_path or \"\\\\p3\\\\\" in img_path:\n                mask_prefix = \"p3-masks/p3\"\n\n            mask_path = img_path.replace(\"Crops data/For_age_prediction\", mask_prefix).replace(\".png\", \"_leaf_masks.npz\")\n\n            if os.path.isfile(mask_path):\n                try:\n                    mask_data = np.load(mask_path)\n                    masks = mask_data['final']\n                    if masks.size > 0:\n                        resized_masks = np.any(masks, axis=0).astype(np.float32)\n                        resized_masks = cv2.resize(resized_masks, (224,224))\n                        mask_tensor = torch.from_numpy(resized_masks).unsqueeze(0)\n                        num_leaves = masks.shape[0]\n                        avg_area = masks.sum() / num_leaves / (224*224)\n                except:\n                    pass\n            num_leaves_map = torch.full((1, 224, 224), num_leaves)\n            avg_area_map = torch.full((1, 224, 224), avg_area)\n            img_combined = torch.cat([img, mask_tensor, num_leaves_map, avg_area_map], dim=0)\n            \n            images.append(img_combined)\n\n        images = torch.cat(images, dim=0)\n\n        return images, torch.tensor(leaf_count, dtype=torch.float32), torch.tensor(age, dtype=torch.float32)  # Return both images and the corresponding day as ground truth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.628947Z","iopub.execute_input":"2025-04-26T23:09:23.629118Z","iopub.status.idle":"2025-04-26T23:09:23.64458Z","shell.execute_reply.started":"2025-04-26T23:09:23.629101Z","shell.execute_reply":"2025-04-26T23:09:23.643987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = CropDataset(root_dir=root_path,\n                      csv_file=csv_file,\n                      images_per_level=n_images,\n                      crop=crop,\n                      plants=plant_input,\n                      days=days_input,\n                      transform=transform)\n\n# Split the dataset into training and validation sets\ntotal_len = len(dataset)\ntrain_len = int(0.7 * total_len)\nval_len = int(0.15 * total_len)\ntest_len = total_len - train_len - val_len\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\nprint(f\"Train: {train_len}, Val: {val_len}, Test: {test_len}\")\n\n# DataLoader for training and validation sets\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.645255Z","iopub.execute_input":"2025-04-26T23:09:23.645507Z","iopub.status.idle":"2025-04-26T23:09:23.873561Z","shell.execute_reply.started":"2025-04-26T23:09:23.645485Z","shell.execute_reply":"2025-04-26T23:09:23.872888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisionTransformerCrossViewSAM(nn.Module):\n    def __init__(self, input_channels_total, num_views, patch_size, projection_dim, num_heads, num_layers, mlp_dim, dropout_rate=0.1):\n        super(VisionTransformerCrossViewSAM, self).__init__()\n        self.num_views = num_views\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(input_channels_total, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, 3, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU()\n        )\n        self.patchify = nn.Conv2d(256, projection_dim, patch_size, patch_size)\n\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=projection_dim,\n                nhead=num_heads,\n                dim_feedforward=mlp_dim,\n                batch_first=True\n            ),\n            num_layers=num_layers\n        )\n\n        self.mlp_head = nn.Sequential(\n            nn.Linear(projection_dim, mlp_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_dim, 1)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        feats = self.cnn(x)\n        b, c, h, w = feats.shape\n        feats = feats.view(b, self.num_views, c//self.num_views, h, w)\n        feats = feats.flatten(3).permute(0, 3, 1, 2).flatten(2)\n        x = self.transformer(feats)\n        x = x.mean(dim=1)\n        return self.mlp_head(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.874238Z","iopub.execute_input":"2025-04-26T23:09:23.874469Z","iopub.status.idle":"2025-04-26T23:09:23.881206Z","shell.execute_reply.started":"2025-04-26T23:09:23.874453Z","shell.execute_reply":"2025-04-26T23:09:23.880458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model():\n    return VisionTransformerCrossViewSAM(\n        input_channels_total=n_images * 6,\n        num_views=n_images,\n        patch_size=16,\n        projection_dim=256,\n        num_heads=8,\n        num_layers=6,\n        mlp_dim=512,\n        dropout_rate=0.1\n    )\n\n# Create two independent instances of the model\nmodel = [create_model().to(device), create_model().to(device)]\n\noptimizer = [optim.Adam(model[0].parameters(), lr=0.0001), optim.Adam(model[1].parameters(), lr=0.0001)]\ncriterion = nn.MSELoss()\nscaler = torch.amp.GradScaler('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:23.88195Z","iopub.execute_input":"2025-04-26T23:09:23.882189Z","iopub.status.idle":"2025-04-26T23:09:24.477307Z","shell.execute_reply.started":"2025-04-26T23:09:23.882169Z","shell.execute_reply":"2025-04-26T23:09:24.476799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_validate(train_loader, val_loader, num_epochs=10):\n    train_losses_leaf, train_losses_age = [], []\n    val_losses_leaf, val_losses_age = [], []\n    train_mae_leaf, train_mae_age = [], []\n    val_mae_leaf, val_mae_age = [], []\n    train_r2_leaf, train_r2_age = [], []\n    val_r2_leaf, val_r2_age = [], []\n\n    scaler = torch.amp.GradScaler('cuda')\n\n    for epoch in range(num_epochs):\n        for i in range(2):\n            model[i].train()\n\n        total_loss = [0.0, 0.0]\n        num_samples = [0, 0]\n        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        all_preds, all_labels = [[], []], [[], []]\n\n        for batch_idx, (images, leaf_labels, age_labels) in enumerate(train_loader_tqdm):\n            images, leaf_labels, age_labels = images.to(device), leaf_labels.to(device), age_labels.to(device)\n\n            for i in range(2):\n                optimizer[i].zero_grad()\n\n                with torch.amp.autocast('cuda'):\n                    preds = model[i](images)\n                    labels = leaf_labels if i == 0 else age_labels\n                    loss = criterion(preds.squeeze(), labels)\n\n                if torch.isnan(loss) or torch.isinf(loss):\n                    continue\n\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer[i])\n                torch.nn.utils.clip_grad_norm_(model[i].parameters(), max_norm=1.0)\n                scaler.step(optimizer[i])\n                scaler.update()\n\n                total_loss[i] += loss.item() * images.size(0)\n                num_samples[i] += images.size(0)\n\n                all_preds[i].extend(preds.squeeze().detach().cpu().numpy())\n                all_labels[i].extend(labels.detach().cpu().numpy())\n\n            train_loader_tqdm.set_postfix({\n                \"Leaf RMSE\": (total_loss[0] / (num_samples[0] + 1e-8))**0.5,\n                \"Age RMSE\": (total_loss[1] / (num_samples[1] + 1e-8))**0.5\n            })\n\n        train_losses_leaf.append((total_loss[0] / (num_samples[0] + 1e-8))**0.5)\n        train_losses_age.append((total_loss[1] / (num_samples[1] + 1e-8))**0.5)\n        train_mae_leaf.append(mean_absolute_error(all_labels[0], all_preds[0]))\n        train_mae_age.append(mean_absolute_error(all_labels[1], all_preds[1]))\n        train_r2_leaf.append(r2_score(all_labels[0], all_preds[0]))\n        train_r2_age.append(r2_score(all_labels[1], all_preds[1]))\n\n        \n        for i in range(2):\n            model[i].eval()\n\n        total_val_loss = [0.0, 0.0]\n        num_val_samples = [0, 0]\n        all_val_preds, all_val_labels = [[], []], [[], []]\n\n        val_loader_tqdm = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n        with torch.no_grad():\n            for val_batch_idx, (images, leaf_labels, age_labels) in enumerate(val_loader_tqdm):\n                images, leaf_labels, age_labels = images.to(device), leaf_labels.to(device), age_labels.to(device)\n\n                for i in range(2):\n                    with torch.amp.autocast('cuda'):\n                        preds = model[i](images)\n                        labels = leaf_labels if i == 0 else age_labels\n                        loss = criterion(preds.squeeze(), labels)\n\n                    total_val_loss[i] += loss.item() * images.size(0)\n                    num_val_samples[i] += images.size(0)\n\n                    all_val_preds[i].extend(preds.squeeze().cpu().numpy())\n                    all_val_labels[i].extend(labels.cpu().numpy())\n\n                val_loader_tqdm.set_postfix({\n                    \"Val Leaf RMSE\": (total_val_loss[0] / (num_val_samples[0] + 1e-8))**0.5,\n                    \"Val Age RMSE\": (total_val_loss[1] / (num_val_samples[1] + 1e-8))**0.5\n                })\n\n        val_losses_leaf.append((total_val_loss[0] / (num_val_samples[0] + 1e-8))**0.5)\n        val_losses_age.append((total_val_loss[1] / (num_val_samples[1] + 1e-8))**0.5)\n        val_mae_leaf.append(mean_absolute_error(all_val_labels[0], all_val_preds[0]))\n        val_mae_age.append(mean_absolute_error(all_val_labels[1], all_val_preds[1]))\n        val_r2_leaf.append(r2_score(all_val_labels[0], all_val_preds[0]))\n        val_r2_age.append(r2_score(all_val_labels[1], all_val_preds[1]))\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Train MAE Leaf: {train_mae_leaf[-1]:.4f}, Train MAE Age: {train_mae_age[-1]:.4f}, R² Leaf: {train_r2_leaf[-1]:.4f}, R² Age: {train_r2_age[-1]:.4f}\")\n        print(f\"Validation - MAE Leaf: {val_mae_leaf[-1]:.4f}, MAE Age: {val_mae_age[-1]:.4f}, R² Leaf: {val_r2_leaf[-1]:.4f}, R² Age: {val_r2_age[-1]:.4f}\")\n\n        torch.save(model[0].state_dict(), f\"radish_vit_leaf_count_{epoch+1}.pth\")\n        torch.save(model[1].state_dict(), f\"radish_vit_age_prediction_{epoch+1}.pth\")\n\n    torch.save(model[0].state_dict(), \"radish_vit_leaf_count.pth\")\n    torch.save(model[1].state_dict(), \"radish_vit_age_prediction.pth\")\n    print(\"✅ Models saved successfully!\")\n\n    # Plotting\n    plt.figure(figsize=(10,5))\n    plt.plot(range(1, num_epochs+1), train_losses_leaf, label='Train Leaf RMSE')\n    plt.plot(range(1, num_epochs+1), val_losses_leaf, label='Validation Leaf RMSE')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"RMSE\")\n    plt.legend()\n    plt.title(\"Leaf Count Training and Validation RMSE\")\n    plt.savefig(\"leaf_training_validation_rmse.png\")\n    plt.savefig(\"leaf_training_validation_rmse.pdf\")\n    plt.close()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(range(1, num_epochs+1), train_losses_age, label='Train Age RMSE')\n    plt.plot(range(1, num_epochs+1), val_losses_age, label='Validation Age RMSE')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"RMSE\")\n    plt.legend()\n    plt.title(\"Age Prediction Training and Validation RMSE\")\n    plt.savefig(\"age_training_validation_rmse.png\")\n    plt.savefig(\"age_training_validation_rmse.pdf\")\n    plt.close()\n\n    print(\"✅ Graphs saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:24.478116Z","iopub.execute_input":"2025-04-26T23:09:24.478364Z","iopub.status.idle":"2025-04-26T23:09:24.496098Z","shell.execute_reply.started":"2025-04-26T23:09:24.478337Z","shell.execute_reply":"2025-04-26T23:09:24.495372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_validate(train_loader, val_loader, num_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:09:24.498091Z","iopub.execute_input":"2025-04-26T23:09:24.498334Z","execution_failed":"2025-04-26T23:10:48.054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_on_test(model, test_loader):\n    model[0].eval()\n    model[1].eval()\n\n    test_loss = [0, 0]\n    all_preds = [[], []]\n    all_labels = [[], []]\n\n    with torch.no_grad():\n        for images, leaf_labels, age_labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n            images = images.to(device)\n            leaf_labels = leaf_labels.to(device)\n            age_labels = age_labels.to(device)\n\n            for i in range(2):\n                with torch.amp.autocast('cuda'):\n                    preds = model[i](images)\n                    loss = criterion(preds.squeeze(), leaf_labels if i == 0 else age_labels)\n\n                test_loss[i] += loss.item()\n                all_preds[i].extend(preds.squeeze().cpu().numpy())\n                all_labels[i].extend((leaf_labels if i == 0 else age_labels).cpu().numpy())\n\n    rmse_leaf = mean_squared_error(all_labels[0], all_preds[0], squared=False)\n    mae_leaf = mean_absolute_error(all_labels[0], all_preds[0])\n    r2_leaf = r2_score(all_labels[0], all_preds[0])\n\n    rmse_age = mean_squared_error(all_labels[1], all_preds[1], squared=False)\n    mae_age = mean_absolute_error(all_labels[1], all_preds[1])\n    r2_age = r2_score(all_labels[1], all_preds[1])\n\n    print(f\"\\n📊 Test Evaluation Results:\")\n    print(f\"Leaf Count - RMSE: {rmse_leaf:.4f}, MAE: {mae_leaf:.4f}, R²: {r2_leaf:.4f}\")\n    print(f\"Age Prediction - RMSE: {rmse_age:.4f}, MAE: {mae_age:.4f}, R²: {r2_age:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-26T23:10:48.054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_on_test(model, test_loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-26T23:10:48.054Z"}},"outputs":[],"execution_count":null}]}